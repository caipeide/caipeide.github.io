<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Peide  Cai | Publications</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üç™</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://caipeide.github.io//">
       <span class="font-weight-bold">Peide</span>   Cai
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              Home
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description"><em>*</em> denotes equal contribution.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">dignet</abbr>
    
  
  </div> -->

  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/dignet.gif" />
  
  </div>

  <div id="cai2021dignet" class="col-sm-8">
    
      <div class="title">DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic Scenarios with Graph Neural Networks</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <em>Peide Cai</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hengli Wang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/sun-yuxiang-dr/" target="_blank">Yuxiang Sun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://facultyprofiles.ust.hk/profiles.php?profile=ming-liu-eelium#publications" target="_blank">Ming Liu</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>under review</em>
      
      
        , 2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract badge grey waves-effect font-weight-light mr-1" role="button">Abs</a>
    
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2011.06775.pdf" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">PDF</a>
      
    
    
      
      <a href="/assets/bibliography/dignet.bib" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Cite</a>
      
    
    
    
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Traditional modular self-driving frameworks scale poorly in new scenarios, which usually require tedious hand-tuning of rules and parameters to maintain acceptable performance in all foreseeable occasions. Therefore, robust and safe self-driving using traditional frameworks is still challenging, especially in complex and dynamic environments. Recently, deep-learning based self-driving methods have shown promising results with better generalization capability but less hand engineering effort. However, most of the previous learning-based methods are trained and evaluated in limited driving scenarios with scattered tasks, such as lane-following, autonomous braking, and conditional driving. In this paper, we propose a graph-based deep network to achieve scalable self-driving that can handle massive traffic scenarios. Specifically, more than 7,000 km of evaluation is conducted in a high-fidelity driving simulator, in which our method can obey the traffic rules and safely navigate the vehicle in a large variety of urban, rural, and highway environments, including unprotected left turns, narrow roads, roundabouts, and pedestrian-rich intersections. The results also show that our method achieves better performance over the baselines in terms of success rate. </p>
    </div>
    

    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">vtgnet</abbr>
    
  
  </div> -->

  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/vtgnet.jpg" />
  
  </div>

  <div id="Cai2020VTGNetAV" class="col-sm-8">
    
      <div class="title">VTGNet: A Vision-based Trajectory Generation Network for Autonomous Vehicles in Urban Environments</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <em>Peide Cai</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/sun-yuxiang-dr/" target="_blank">Yuxiang Sun</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hengli Wang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://facultyprofiles.ust.hk/profiles.php?profile=ming-liu-eelium#publications" target="_blank">Ming Liu</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Intelligent Vehicles (T-IV)</em>
      
      
        , 2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract badge grey waves-effect font-weight-light mr-1" role="button">Abs</a>
    
    
    
    
    
    
    
      
      <a href="/assets/bibliography/vtgnet.bib" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Cite</a>
      
    
    
    
    
    <a href="https://doi.org/10.1109/TIV.2020.3033878" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">DOI</a>
      
    
      <a href="https://github.com/caipeide/VTGNet" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Traditional methods for autonomous driving are implemented with many building blocks from perception, planning and control, making them difficult to generalize to varied scenarios due to complex assumptions and interdependencies. Recently, the end-to-end driving method has emerged, which performs well and generalizes to new environments by directly learning from export-provided data. However, many existing methods on this topic neglect to check the confidence of the driving actions and the ability to recover from driving mistakes. In this paper, we develop an uncertainty-aware end-to-end trajectory generation method based on imitation learning. It can extract spatiotemporal features from the front-view camera images for scene understanding, and then generate collision-free trajectories several seconds into the future. The experimental results suggest that under various weather and lighting conditions, our network can reliably generate trajectories in different urban environments, such as turning at intersections and slowing down for collision avoidance. Furthermore, closed-loop driving tests suggest that the proposed method achieves better cross-scene/platform driving results than the state-of-the-art (SOTA) end-to-end control method, where our model can recover from off-center and off-orientation errors and capture 80% of dangerous cases with high uncertainty estimations.</p>
    </div>
    

    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">multimodal</abbr>
    
  
  </div> -->

  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/multimodal.gif" />
  
  </div>

  <div id="Cai2020ProbabilisticEV" class="col-sm-8">
    
      <div class="title">Probabilistic End-to-End Vehicle Navigation in Complex Dynamic Environments With Multimodal Sensor Fusion</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <em>Peide Cai</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sukai Wang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/sun-yuxiang-dr/" target="_blank">Yuxiang Sun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://facultyprofiles.ust.hk/profiles.php?profile=ming-liu-eelium#publications" target="_blank">Ming Liu</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Robotics and Automation Letters (RA-L) &amp;<br />IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS‚Äã)</em>
      
      
        , 2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract badge grey waves-effect font-weight-light mr-1" role="button">Abs</a>
    
    
    
    
    
      <a href="https://www.youtube.com/watch?v=xHuRziZ0wn0" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Video</a>
    
    
      
      <a href="https://arxiv.org/pdf/2005.01935.pdf" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">PDF</a>
      
    
    
      
      <a href="/assets/bibliography/multimodal.bib" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Cite</a>
      
    
    
    
    
    <a href="https://doi.org/10.1109/LRA.2020.2967299" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">DOI</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Drifting is a complicated task for autonomous vehicle control. Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely. We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic. The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed. After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map. The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.</p>
    </div>
    

    
  </div>
</div>
</li>
<li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">drift</abbr>
    
  
  </div> -->

  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/drift.gif" />
  
  </div>

  <div id="Cai2020HighSpeedAD" class="col-sm-8">
    
      <div class="title">High-Speed Autonomous Drifting With Deep Reinforcement Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Peide Cai*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaodong Mei*,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://onlytailei.github.io/" target="_blank">Lei Tai</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/sun-yuxiang-dr/" target="_blank">Yuxiang Sun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://facultyprofiles.ust.hk/profiles.php?profile=ming-liu-eelium#publications" target="_blank">Ming Liu</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Robotics and Automation Letters (RA-L) &amp;<br />IEEE International Conference on Robotics and Automation (ICRA)</em>
      
      
        , 2020
      
      
        <b>(Note)</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract badge grey waves-effect font-weight-light mr-1" role="button">Abs</a>
    
    
    
    
    
      <a href="https://youtu.be/ZH-ZbK5dLs0" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Video</a>
    
    
      
      <a href="https://arxiv.org/pdf/2005.01935.pdf" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">PDF</a>
      
    
    
      
      <a href="/assets/bibliography/drift.bib" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Cite</a>
      
    
    
    
    
    <a href="https://doi.org/10.1109/LRA.2020.2967299" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">DOI</a>
      
    
      <a href="https://github.com/caipeide/drift_drl" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Drifting is a complicated task for autonomous vehicle control. Most traditional methods in this area are based on motion equations derived by the understanding of vehicle dynamics, which is difficult to be modeled precisely. We propose a robust drift controller without explicit motion equations, which is based on the latest model-free deep reinforcement learning algorithm soft actor-critic. The drift control problem is formulated as a trajectory following task, where the error-based state and reward are designed. After being trained on tracks with different levels of difficulty, our controller is capable of making the vehicle drift through various sharp corners quickly and stably in the unseen map. The proposed controller is further shown to have excellent generalization ability, which can directly handle unseen vehicle types with different physical properties, such as mass, tire friction, etc.</p>
    </div>
    

    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <!-- <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">vision_plan</abbr>
    
  
  </div> -->

  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/vision_planner.gif" />
  
  </div>

  <div id="Cai2019Vision" class="col-sm-8">
    
      <div class="title">Vision-Based Trajectory Planning via Imitation Learning for Autonomous Vehicles</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <em>Peide Cai</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/sun-yuxiang-dr/" target="_blank">Yuxiang Sun</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuying Chen,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://facultyprofiles.ust.hk/profiles.php?profile=ming-liu-eelium#publications" target="_blank">Ming Liu</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE Intelligent Transportation Systems Conference (ITSC)</em>
      
      
        , 2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract badge grey waves-effect font-weight-light mr-1" role="button">Abs</a>
    
    
    
    
    
    
    
      
      <a href="/assets/bibliography/vision_plan.bib" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">Cite</a>
      
    
    
    
    
    <a href="https://doi.org/10.1109/ITSC.2019.8917149" class="badge grey waves-effect font-weight-light mr-1" role="button" target="_blank">DOI</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reliable trajectory planning like human drivers in real-world dynamic urban environments is a critical capability for autonomous driving. To this end, we develop a vision and imitation learning-based planner to generate collision-free trajectories several seconds into the future. Our network consists of three sub-networks to conduct three basic driving tasks: keep straight, turn left and turn right. During the planning process, high-level commands are received as prior information to select a specific sub-network. We create our dataset from the Robotcar dataset, and the experimental results suggest that our planner is able to reliably generate trajectories in various driving tasks, such as turning at different intersections, lane-keeping on curved roads and changing lanes for collision avoidance.</p>
    </div>
    

    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Peide  Cai.
    Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
    Last updated: June 25, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
